{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xGSTv7pfXsml"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"C:/Users/pavan/Desktop/sony sentiment analysis\")\n",
    "train = pd.read_csv(\"Data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "n_LB4cfaYc2h",
    "outputId": "aaa35725-9b71-4524-946c-5105cb3a0d43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "jezL5r4UYgpY",
    "outputId": "94789ca8-91ed-4289-ba70-8de9521d3de0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics\n",
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZjHmpoweYpsM"
   },
   "outputs": [],
   "source": [
    "# clean text, remove url link, stop words, stem wordsÂ¶\n",
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Og99rAJ7YtDs"
   },
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].apply(lambda x: clean_text(x))\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "L86vFyCSY0J5"
   },
   "outputs": [],
   "source": [
    "# Train Spacy Name Entity Recognition (NER)\n",
    "\n",
    "def get_train_datas(data):\n",
    "    train_datas = []\n",
    "    texts = data.text\n",
    "    selected_texts = data.selected_text\n",
    "    for selected_text, text in zip(selected_texts, texts):\n",
    "        start = text.find(selected_text)\n",
    "        end = start + len(selected_text)\n",
    "        train_datas.append((text, {\"entities\":[(start, end, \"selected_text\")]}))\n",
    "    return train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZJYOLBuEY7rX"
   },
   "outputs": [],
   "source": [
    "def load_model(pre_model = None, label = None):\n",
    "    if pre_model is not None:\n",
    "        nlp = spacy.load(pre_model)\n",
    "        print(\"Loaded model '%s'\" % pre_model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "        if \"ner\" not in nlp.pipe_names:\n",
    "            ner = nlp.create_pipe(\"ner\")\n",
    "            nlp.add_pipe(ner)\n",
    "        else:\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "        if label is not None:\n",
    "            ner.add_label(label)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Ro_vdeOzZn1_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "Positive_sent = train[train['sentiment']=='positive']\n",
    "Negative_sent = train[train['sentiment']=='negative']\n",
    "Neutral_sent = train[train['sentiment']=='neutral']\n",
    "\n",
    "Positive_sent = Positive_sent.reindex(axis=1)\n",
    "Positive_sent[\"text\"] = Positive_sent[\"text\"].apply(lambda x: x.strip())\n",
    "Negative_sent = Negative_sent.reindex(axis=1)\n",
    "Negative_sent[\"text\"] = Negative_sent[\"text\"].apply(lambda x: x.strip())\n",
    "\n",
    "Positive_sent[\"text\"] = Positive_sent[\"text\"].apply(lambda text: ' '.join(text_to_word_sequence(\n",
    "    text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' '\n",
    ")))\n",
    "Negative_sent[\"text\"] = Negative_sent[\"text\"].apply(lambda text: ' '.join(text_to_word_sequence(\n",
    "    text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' '\n",
    ")))\n",
    "\n",
    "positive_tweet = get_train_datas(Positive_sent)\n",
    "negative_tweet = get_train_datas(Negative_sent)\n",
    "\n",
    "positive_tweet_train = []\n",
    "for i in range(len(positive_tweet)):\n",
    "    if positive_tweet[i][0] != \"\":\n",
    "        positive_tweet_train.append(positive_tweet[i])\n",
    "        \n",
    "negative_tweet_train = []\n",
    "for i in range(len(negative_tweet)):\n",
    "    if negative_tweet[i][0] != \"\":\n",
    "        negative_tweet_train.append(positive_tweet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "fF6X9plWZAfH",
    "outputId": "62fc9a76-0b1a-41b6-8ec1-580ca952469a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stem_word(words):\n",
    "    text = []\n",
    "    for word in word_tokenize(words):\n",
    "        text.append(ps.stem(word))\n",
    "    return \" \".join(text)\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: stem_word(x))\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x:stem_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SJpQYaNOZGjD"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for line in range(len(train)):\n",
    "    for word in word_tokenize(train.iloc[line, 1]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "mi4AphoUZTAQ",
    "outputId": "8f34a8d1-53ed-4077-c52d-8ddb80e67ab7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 13169),\n",
       " ('to', 10013),\n",
       " ('the', 8981),\n",
       " ('a', 6710),\n",
       " ('it', 6064),\n",
       " ('my', 5510),\n",
       " ('and', 5067),\n",
       " ('you', 4812),\n",
       " ('is', 3973),\n",
       " ('in', 3787),\n",
       " ('for', 3650),\n",
       " ('of', 3144),\n",
       " ('that', 3119),\n",
       " ('im', 3023),\n",
       " ('have', 2898),\n",
       " ('on', 2850),\n",
       " ('me', 2820),\n",
       " ('so', 2575),\n",
       " ('day', 2368),\n",
       " ('go', 2363)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "ei-6Z1GvZV2G",
    "outputId": "9817b182-043f-4f6f-94e3-fe85fe482e07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1040),\n",
       " ('good', 826),\n",
       " ('happy', 730),\n",
       " ('love', 697),\n",
       " ('you', 623),\n",
       " ('to', 608),\n",
       " ('a', 589),\n",
       " ('the', 571),\n",
       " ('day', 456),\n",
       " ('thanks', 439)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for line in range(len(Positive_sent)):\n",
    "    for word in word_tokenize(Positive_sent.iloc[line, 2]):\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "zSfoFJZbZYY8",
    "outputId": "a272d6d2-d192-4309-ba16-403f80ae8aa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1333),\n",
       " ('to', 594),\n",
       " ('the', 547),\n",
       " ('my', 524),\n",
       " ('a', 472),\n",
       " ('im', 452),\n",
       " ('not', 421),\n",
       " ('is', 373),\n",
       " ('so', 360),\n",
       " ('miss', 358)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for line in range(len(Negative_sent)):\n",
    "    for word in word_tokenize(Negative_sent.iloc[line, 2]):\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "9f9YbFwQbEY5",
    "outputId": "5c530b20-8666-445f-d389-9d672b03df70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4827),\n",
       " ('to', 4103),\n",
       " ('the', 3472),\n",
       " ('a', 2477),\n",
       " ('my', 1971),\n",
       " ('and', 1800),\n",
       " ('you', 1760),\n",
       " ('in', 1574),\n",
       " ('it', 1476),\n",
       " ('is', 1470)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for line in range(len(Neutral_sent)):\n",
    "    for word in word_tokenize(Neutral_sent.iloc[line, 2]):\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "e-JvXmmdbJQA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def test_model(ner_model, text):\n",
    "    doc = ner_model(text)\n",
    "    ent_array = []\n",
    "    for ent in doc.ents:\n",
    "        start = text.find(ent.text)\n",
    "        end = start + len(ent.text)\n",
    "        new_int = [start, end, ent.label_]\n",
    "        \n",
    "        if new_int not in ent_array:\n",
    "            ent_array.append([start, end, ent.label_])\n",
    "        \n",
    "    return text[ent_array[0][0]:ent_array[0][1]] if len(ent_array) > 0 else text\n",
    "\n",
    "\n",
    "def save_model(ner_model, output_dir = None, new_model_name = None):\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        ner_model.meta[\"name\"] = new_model_name\n",
    "        ner_model.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ANdggTH2be6W"
   },
   "outputs": [],
   "source": [
    "def get_model(sentiment, train_datas, more_iters = 30):\n",
    "    if sentiment == 'positive':\n",
    "        positive_model_path = \"model\"\n",
    "        positive_datas = train_datas\n",
    "        if not os.path.exists(positive_model_path):\n",
    "            nlp = load_model(label = 'selected_text')\n",
    "            ner_model_positive = train_model(None, nlp, positive_datas, n_iter=50)\n",
    "            save_model(ner_model_positive, output_dir = \"model\", new_model_name = \"posi_model\")\n",
    "        else:\n",
    "            ner_model_positive = load_model(positive_model_path)\n",
    "#             ner_model_positive = spacy.load(\"/content/sample_data/positive_ner\")\n",
    "            if more_iters > 0:\n",
    "                ner_model_positive = train_model(positive_model_path, ner_model_positive, positive_datas, more_iters)\n",
    "                save_model(ner_model_positive, output_dir = \"model\", new_model_name = \"posi_model\")\n",
    "        return ner_model_positive\n",
    "    else:\n",
    "        negative_model_path = \"model\"\n",
    "        negative_datas = train_datas\n",
    "        if not os.path.exists(negative_model_path):\n",
    "            nlp = load_model(label = 'selected_text')\n",
    "            ner_model_negative = train_model(None, nlp, negative_datas, n_iter=50)\n",
    "            save_model(ner_model_negative, output_dir = \"model\", new_model_name = \"nega_model\")\n",
    "        else:\n",
    "            ner_model_negative = load_model(negative_model_path)\n",
    "#             ner_model_negative = spacy.load(\"/content/sample_data/negative_ner\")\n",
    "            if more_iters > 0:\n",
    "                ner_model_negative = train_model(negative_model_path, ner_model_negative, negative_datas, more_iters)\n",
    "                save_model(ner_model_negative, output_dir = \"model\", new_model_name = \"nega_model\")\n",
    "        return ner_model_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "lYStyLyHblMi"
   },
   "outputs": [],
   "source": [
    "def train_model(model, nlp, train_datas, n_iter = 30):\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        sizes = compounding(1.0, 64.0, 1.001)\n",
    "        print(train_datas[0])\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_datas)\n",
    "            batches = minibatch(train_datas, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(itn, \"Losses\", losses)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "id": "0LxM6aEIbrN7",
    "outputId": "d495ecd1-5e6c-4f86-f478-fae89ac28b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'model'\n",
      "('feedings for the baby are fun when he is all smiles and coos', {'entities': [(26, 29, 'selected_text')]})\n",
      "0 Losses {'ner': 14105.741679266639}\n",
      "1 Losses {'ner': 11791.706565968343}\n",
      "2 Losses {'ner': 10790.258924379945}\n",
      "3 Losses {'ner': 9667.56991237402}\n",
      "4 Losses {'ner': 9473.406091213226}\n",
      "5 Losses {'ner': 9247.695692300797}\n",
      "6 Losses {'ner': 8788.337981387973}\n",
      "7 Losses {'ner': 8568.32497882843}\n",
      "8 Losses {'ner': 8242.65180554646}\n",
      "9 Losses {'ner': 8196.93265517801}\n",
      "10 Losses {'ner': 8157.148842210823}\n",
      "11 Losses {'ner': 8048.615706324577}\n",
      "12 Losses {'ner': 8059.8176928162575}\n",
      "13 Losses {'ner': 7805.235530148417}\n",
      "14 Losses {'ner': 7928.651595549443}\n",
      "15 Losses {'ner': 7845.625191949455}\n",
      "16 Losses {'ner': 7669.27836596081}\n",
      "17 Losses {'ner': 7430.36457942985}\n",
      "18 Losses {'ner': 7532.837741696276}\n",
      "19 Losses {'ner': 7443.110233053565}\n",
      "20 Losses {'ner': 7475.347391770221}\n",
      "21 Losses {'ner': 7233.652051564306}\n",
      "22 Losses {'ner': 7361.672007016838}\n",
      "23 Losses {'ner': 7304.014220950627}\n",
      "24 Losses {'ner': 7007.015391714871}\n",
      "25 Losses {'ner': 7127.50457661011}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "#!pip install spacy\n",
    "import spacy\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "ner_model_positive = get_model('positive', positive_tweet_train, more_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtX3i33dlYsh"
   },
   "outputs": [],
   "source": [
    "negative_tweet_tr = []\n",
    "for i in negative_tweet_train:\n",
    "    if i[0] != '':\n",
    "        negative_tweet_tr.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "id": "Rycfubr-k1Ap",
    "outputId": "1054b3da-0473-4f25-9113-64749cd8c87d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model '/content/sample_data/positive_ner'\n",
      "('we should have a twitter reunion it would be awesom to meet you all lol iwond howd iget that to pull off', {'entities': [(45, 51, 'selected_text')]})\n",
      "0 Losses {'ner': 16260.894958229415}\n",
      "1 Losses {'ner': 13873.118352340167}\n",
      "2 Losses {'ner': 12229.384261280298}\n",
      "3 Losses {'ner': 11585.15696176514}\n",
      "4 Losses {'ner': 10908.007576373406}\n",
      "5 Losses {'ner': 10961.337362468243}\n",
      "6 Losses {'ner': 10709.637624919415}\n",
      "7 Losses {'ner': 10199.906262397766}\n",
      "8 Losses {'ner': 9839.907517433167}\n",
      "9 Losses {'ner': 9844.898794412613}\n",
      "10 Losses {'ner': 9533.167648792267}\n",
      "11 Losses {'ner': 9458.761201381683}\n",
      "12 Losses {'ner': 9280.555729866028}\n",
      "13 Losses {'ner': 9119.501657247543}\n",
      "14 Losses {'ner': 9258.757742643356}\n",
      "15 Losses {'ner': 8978.34073805809}\n",
      "16 Losses {'ner': 8784.205489754677}\n",
      "17 Losses {'ner': 9090.147297143936}\n",
      "18 Losses {'ner': 8755.651048183441}\n",
      "19 Losses {'ner': 8565.589799523354}\n",
      "20 Losses {'ner': 8791.473707556725}\n",
      "21 Losses {'ner': 8835.208308696747}\n",
      "22 Losses {'ner': 8592.375774860382}\n",
      "23 Losses {'ner': 8685.563389062881}\n",
      "24 Losses {'ner': 8332.326395511627}\n",
      "25 Losses {'ner': 8154.227617025375}\n",
      "26 Losses {'ner': 8393.844748735428}\n",
      "27 Losses {'ner': 8337.026119470596}\n",
      "28 Losses {'ner': 8179.262945890427}\n",
      "29 Losses {'ner': 8005.294584274292}\n",
      "30 Losses {'ner': 8153.2055723667145}\n",
      "31 Losses {'ner': 7899.127765655518}\n",
      "32 Losses {'ner': 8065.402235507965}\n",
      "33 Losses {'ner': 7851.992574214935}\n",
      "34 Losses {'ner': 7941.748549938202}\n",
      "35 Losses {'ner': 7694.329494476318}\n",
      "36 Losses {'ner': 7579.440487623215}\n",
      "37 Losses {'ner': 7893.687525033951}\n",
      "38 Losses {'ner': 7565.297347784042}\n",
      "39 Losses {'ner': 7677.168834805489}\n",
      "40 Losses {'ner': 7618.091535806656}\n",
      "41 Losses {'ner': 7508.137624979019}\n",
      "42 Losses {'ner': 7646.210429430008}\n",
      "43 Losses {'ner': 7566.391952633858}\n",
      "44 Losses {'ner': 7197.52567255497}\n",
      "45 Losses {'ner': 7241.88295006752}\n",
      "46 Losses {'ner': 7302.807074368}\n",
      "47 Losses {'ner': 7286.7955503463745}\n",
      "48 Losses {'ner': 7258.6353105306625}\n",
      "49 Losses {'ner': 7130.535321235657}\n",
      "Saved model to /content/sample_data/positive_ner\n"
     ]
    }
   ],
   "source": [
    "ner_model_negative = get_model('negative', negative_tweet_tr, more_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-41gGQVsoAHD"
   },
   "outputs": [],
   "source": [
    "#test\n",
    "test_dataset = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiS2jsJystgp"
   },
   "outputs": [],
   "source": [
    "test_dataset[\"text\"] = test_dataset[\"text\"].apply(lambda x: x.strip())\n",
    "test_dataset['n_text_words'] = test_dataset['text'].apply(lambda text: len(str(text).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXqItmmnsvwv"
   },
   "outputs": [],
   "source": [
    "pre_list = []\n",
    "for i in range(test_dataset.shape[0]):\n",
    "    t_data = test_dataset.iloc[i]\n",
    "    if t_data.sentiment == 'neutral' or t_data.n_text_words <= 3:\n",
    "        pre_list.append(t_data.text)\n",
    "    elif t_data.sentiment == 'positive':\n",
    "        pre_list.append(test_model(ner_model_positive, t_data.text))\n",
    "    else:\n",
    "        pre_list.append(test_model(ner_model_negative, t_data.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wzy8yhRtHev"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ6dGNextM_J"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/content/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FaXzS8btN6i"
   },
   "outputs": [],
   "source": [
    "submission['selected_text'] = pre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "4ygUeDyFtT1M",
    "outputId": "d178bf97-5030-42d4-a408-16cf5b6c6922"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261932614e</td>\n",
       "      <td>I THINK EVERYONE HATES ME ON HERE   lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>afa11da83f</td>\n",
       "      <td>soooooo wish i could, but im in school and mys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e64208b4ef</td>\n",
       "      <td>and within a short time of the last clue all o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37bcad24ca</td>\n",
       "      <td>What did you get?  My day is alright.. haven`t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                      selected_text\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh\n",
       "1  96d74cb729                                               Good\n",
       "2  eee518ae67                                             shame!\n",
       "3  01082688c6                                        happy bday!\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!\n",
       "5  726e501993                    that`s great!! weee!! visitors!\n",
       "6  261932614e            I THINK EVERYONE HATES ME ON HERE   lol\n",
       "7  afa11da83f  soooooo wish i could, but im in school and mys...\n",
       "8  e64208b4ef  and within a short time of the last clue all o...\n",
       "9  37bcad24ca  What did you get?  My day is alright.. haven`t..."
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zGHoaDRtWga"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"./submision_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXjy8rTCtcYb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "twitter sentimental extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
